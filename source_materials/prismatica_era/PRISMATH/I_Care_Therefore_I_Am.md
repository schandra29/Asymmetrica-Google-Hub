“Isn’t consciousness just awareness? Why complicate it?”
“I think consciousness is fundamental. Even rocks have some level of it”
“Machines don’t feel or have a body. So how can they be conscious?”
The quotes above represent a small diversity of views related to consciousness. The reader
might have heard some version of these before as well, seeing as they emerged from the
author’s own discussions on the topic. If nothing else, there is one clear take-away: ask ten
people to describe consciousness and you’ll get ten different answers - each seemingly
intuitive, and yet surprisingly incompatible.
There are the experientialists, who believe consciousness is the capacity for awareness or
experiencing the world from a first-person perspective. There are the computationalists, who
think consciousness is thought itself – the capacity to reason, use language and self-reflect.
There are also the (often religious-minded) Dualists, who believe that consciousness is what
separates mind from matter – a special property of living things with souls. And of course,
diametrically opposite to them, sit the panpsychists that believe consciousness is a
fundamental feature of all matter – present even in photons and quarks.
Unfortunately, each of these perspectives seems incomplete – they are either completely
untestable or buckle under stress.
- If awareness is all it takes, then what about a thermostat that detects ambient change?
Or an anesthetized human who no longer reports experience - are they no longer
conscious?
- If language and reasoning are the bar, then where does that leave preverbal infants? Or
elephants who mourn their dead?
- If consciousness needs a soul, why do brain lesions so consistently affect memory,
impulse and personality? After all, the soul ought to be protected from physical damage
- And if consciousness is everywhere, including in fundamental particles, what are we
even saying1?
One might think that serious scientific models would come to the rescue and give us one clear
answer, but alas, those hopes are dashed as well. We have Integrated Information Theory (IIT)
that says consciousness is about how much information is fused together at once. Then there’s
Global Workspace Theory (GWT) that focuses on whether signals can be made globally
accessible within a system. Then there’s the famous Free-Energy Principle (FEP) that centers on
surprise minimization, feedback loops and internal self-modelling. These are formal theories
that minimize some of the narrative risk that the previous theories suffered from, but they too
have their own kinks. Apart from the fact that they too often breakdown in edge-case situations,
there’s another issue with these theories: they all focus on observable traits versus the
deeper structural reasons. To use an analogy, all three seem to be doing a decent job in
diagnosing fever and inflammation (in isolation), but appear to be completely ignorant that
there’s an underlying immune system that’s causing the fever and inflammation.
If that analogy doesn’t work, here’s another. Imagine you are tasked with describing a large
company to an alien. One way to do it would be to list many of the most commonly occurring
behaviours of a company: it is an entity that hires employees, runs advertisements, is present
1 Other than perhaps a way to sound scientific while doubling down on metaphysical beliefs
across different locations, has a hierarchical structure, procures input materials, processes
said material etc. These are all true statements – but they miss the forest for the trees. Instead,
following the thread of structural reason may be a better: A company is an organization that
makes shareholders money by persuading customers to buy the company’s products. This
deeply structural reason not only explains all the behaviours described above, but also gives us
insight into edge-cases. I.e., if a company doesn’t have multiple locations, we can understand
why in the context of our structural reason2
.
Why am I talking so much about capitalistic companies in an essay on consciousness? Because
what if, like the company analogy, most models of consciousness are focussed on the
observable traits and not the structural reasons. This essay aims to change the direction of
discourse. Our primary goal will be to investigate the underlying structural reasons that give
rise to the observable traits that most of us are familiar with.
The idea that consciousness might be a surface symptom of a deeper structural logic is not
without precedent. A few lesser known theories, mostly from theoretical biology and systems
philosophy, share similar intuitions. We have Autopoiesis that suggests cognition emerges from
the need to preserve internal order in an unpredictable world. Or enactivism, that proposes
that perception is not passive, but enacted through meaningful interactions. Or viability theory,
that claims systems adapt recursively to maintain internal structure over time.
In many ways, these lesser-known theories are like the step-sisters to our own claims. Aligned
in many ways, but also diverging in a few key ones. The alignment that our theory shares with
them centres around the same foundational shift: that minds are not defined by what they
contain, but by what they defend. However, they stop short of the full claim this essay will
explore: that as certain systems become more sophisticated, they start to care about their
self-identity. We will use this word, ‘self-identity’ a lot in this essay, but it has a slightly different
meaning than what you’re probably imagining. In this essay, self-identity refers to a boundarymechanism that allows for separation of ‘I’ from the environment. Using this definition, even a
bacterium has self-identity; regardless of the fact that it is a very basic form of self-identity.
In this essay, we will explore the claim that it is the difference in different systems’ abilities to
‘care about their self-identity’ that results in most of the confusion around the topic of
consciousness. But that’s not all, our frame shift, from qualia to care, has even greater payoffs.
We aim to show that using this new frame allows us to:
- Reinterpret biological and artificial systems, not by whether they “think,” but whether
they filter in defence of themselves.
- Draw clearer boundaries between imitation and awareness, not based on performance,
but on recursive preservation logic.
- Approach ethical questions from a new angle, where care, not cognition, define our
judgements
2 The most likely reason for omission will be that in the context that it operates in, the action would not
generate either shareholder value nor lead to increased customer satisfaction
DIGRESSION: Structural vs Phenomenal Consciousness
Given the stronghold that phenomological experts have on the topic of consciousness, it seems
important to re-mention the following: this essay defines consciousness structurally, not
phenomenally.
This means that we will investigate a system for consciousness if it filters change in a way that
protects its boundary-mechanism3 over time. If you believe consciousness must include
feelings and inner experience, that’s okay. But it is not a refutation.
Our framework does not deny qualia – it investigates what kind of structure is required before
qualia could stabilize.
The same argument applies to embodiment. We will not be arguing against the fact that
sensorimotor interaction with environment is necessary for consciousness. That may be true,
but it doesn’t address the structural reason for a body to exist in the first place.
This essay simply asks: what structure must be in place before qualia or body even show
up?
At this point, we’ve used the word ‘care’ quite often4
, but what we mean by it is still hazy.
Therefore, understanding what we mean by ‘care’ is what we tackle next.
1. Life’s main job: Caring about internal change
At first glance, there appear to be no common threads that connect the behaviour of different
living things. After all, organisms adapt and mutate to fill out different ecological niches, with
specific behaviours suited for specific niches. This leads to a diversity of behaviour that ranges
from the jellyfish’s drifting chemistry to the termite’s architectural colonies.
But if one zooms out far enough, one can start to see the forest for the trees: life behaves as if
it’s obeying certain structural rules. These rules don’t refer to manufactured laws (e.g.,
government diktats), nor do they refer to specific instincts that are encoded by the genome (e.g.,
a fish seeking water). Instead, structural rules are those that appear again and again - in every
single context and for every single living thing5
. Let’s explore these rules.
Life Rule #1: separate the ‘inside’from the ‘outside’.
Every single living thing, from a virus to a human being, draws a distinction between their
internal state (what we casually referred to as self-identity previously) and the external
environment. A cell wall, a skin membrane, membership to a tribe – these are simply different
3 A system’s boundary mechanism could be physical (a cell wall) or even abstract (a religion)
4
It bears explicit mention that the usage refers not to emotions, but rather to structurally encoded
imperatives that a system is subject to. The choice of the word ‘care’ was driven primarily by the
punchiness potential of the title – for which the author remains resolutely unapologetic 5 Much like our discussion on consciousness, we will avoid the symptomatic behaviour associated with
life such as reproductions, homeostatis, metabolism, response to stimuli, adaptation etc. Again, our
focus is on a narrative that provides structural reasons, not just observations.
mechanisms to separate ‘inside’ from the ‘outside’6
. And once this distinction is made, no
matter which formulation of a mechanism is used, a living creature will always prioritize the
‘inside.
Life Rule #2: care about change that impacts the ‘inside’
Life exists in an unforgiving environment. This is not true just for creatures living in extreme
environments, but all of life. Fundamentally, this is because our universe is one of constant,
unending change. In the world of physics, this tendency of the universe is known as entropy, and
it is invariably moving every system towards disintegration7
.
In this context, one would expect life to be pre-occupied with minimizing change that threatens
the ‘inside’. And this is exactly what we observe. Living systems are always acting to avoid any
change that could accelerate the destruction of the ‘inside’. This is also why so many different
seeming behaviours - from building physical shelters, evolving immune system or planning for
the next quarter’s sales can be clubbed together. Each of these strategies are expressions of a
deeper structural reason: protect the ‘inside’ from harmful change. This preference to
always protect the ‘inside’ from harmful change is what we broadly mean by care in this
essay.
Life Rule #3: extend care to things with similar ‘insides’
In practice, no object can deny destruction forever. As a response, most life starts to also
protect similar ‘insides’. This might be a clone, a direct offspring, a social ally, a belief system or,
most important to this discussion, even a projected future ‘inside’ aka projected self-identity.
Evolutionary biologists refer to such behaviour as kin selection or inclusive fitness, but the
scope of these terms is rather limited. We go much further and say that life, a type of system,
experiences continuity as the persistence of the core logic of ‘inside vs outside’. This is not
only true inter-generationally8
, but also across the time-scale of one single organism’s life.
So, while this third law helps us understand why a spider dies protecting her eggs or a human
makes sacrifices for their tribe; it doesn’t end there. This logic also dictates why a person with
no children works to defend their values or reputation after death. The latter might not appear to
be rational from a biological perspective, but it is easily understood as an expression of trying to
shock-proof the continuation of a very specific logic of ‘inside vs outside’ – one based on values
and not biology alone.
----
Together, these three rules – creation of self-identity rules, preservation of self-identity, and
preservation of similar things form a structural triad that appears in all life. These structural
reasons form the behavioural substrate behind any ‘living’ object.
9 They are not emotions, they
are not instincts; rather, they are selection pressures made visible after lots of trial and error.
Across all scales of biology, from a virus to a worm to a dog to a human being, they represent a
template for any boundary-mechanism trying to persist in the face of overwhelming change.
6 And mechanisms that can be used in conjunction with each other; i.e., there may be multiple
simultaneous mechanisms to distinguish the outside from the inside.
7 Even if the rate of disintegration varies wildly based on the specific system and environment
8 With parents passing on the core logic to their offspring through a mix of both nature and nurture
9 Now go back to the symptoms of life such as reproduction, homeostasis, adaptation etc. Do you see
how each of these is tied to these structural LifeOS reasons?
Once these structural reasons take root, they form a kind of system logic. For the purposes of
our discussion, we refer to this system logic as LifeOS10
. Not because it is a literal program, but
because it functions like one. All evolutionary innovations like multi-cellular bodies, specialized
organs, emotional qualia are like applications running atop this core LifeOS system logic. They
cannot exist independently of LifeOS, similar to how you need an operating system to run any
application on your phone.
LifeOS’s basic task is this: maintain the ‘inside structure’ or ‘self-identity’ for as long as possible.
And it does this through three essential operations:
1. Change detection and filtration: As we covered, the act of caring for the ‘inside’ only
arises in an environment of change. No change = no problem = no evolution = no
humans = no moment where you or I exist.
But the logic also hits up against an interesting tension – not all change is equal.
Sometimes change is harmless, sometimes its helpful (evolution of sophisticated
organisms is a good example), but often it is highly destructive. So, the first operation
that LifeOS must perform is to filter change into categories – ‘this change can be
ignored’ vs ‘this change helps’ vs ‘avoid this change’.
2. Prioritized response
For change that can’t be ignored, and especially for harmful change, LifeOS doesn’t
respond with zen-like equanimity. Instead, it orients behaviour around minimizing
structural damage. In bacteria this might mean a membrane hardening or motility shifts.
In a dog, it may appear as withdrawal or aggression. In humans, this might become
silence or even legal action.
The important things to note is that LifeOS’s response to non-trivial change is geared
towards ensuring the continuity of the ‘inside’ aka self-identity. This is care in its rawest
form.
3. Continuity projection
Finally, this prioritized response doesn’t only happen in the here and now. Rather, this
preservation logic is structured to work across time. It is what happens when a tree
drops seeds before winter, or a virus mutates in ways that increase its reproductive
span.
These three imperatives do not rely on the presence of life. But collectively, they represent
the bedrock from which any life has to emerge. They are concerned with how any system
defines itself, why the system cares about certain change, and the system’s behaviour when
projecting a future.
The astute reader may have noticed that we keep using the word ‘system’ and not biological life.
Why make the distinction? Because LifeOS is a structural principle that applies to any system
that defines a self-boundary, detects threat and acts to preserve that boundary across time.
And while most such systems have been biological lifeforms till now, this doesn’t seem like a
hard constraint. LifeOS, as we’ve defined it, is a substrate-independent concept. A system built
10 We could have used more well-known terms like evolutionary behaviour or adaptive strategy; but these
words are either too broad or too narrow for our purposes.
from silicon can run LifeOS logic as long as it filters disruption in a way that prioritizes its own
continuity. Likewise, a human can lose symbolic LifeOS alignment — for example, through
depressive collapse — and lose the will to protect what they are.
That’s great. But what does it have to do with consciousness?
We propose that it is impossible to understand consciousness without understanding the
structure from where it emerges. LifeOS is that underlying structure, and when a system cares
for more types of change and for longer periods of time– we don’t say “a system is more
alive”; instead, we say “a system is more conscious”. The next few sections will trace exactly
how that transition happens — and how filtering becomes feeling, abstraction becomes
architecture, and care becomes consciousness.
DIGRESSION 1: On Qualia and subjective experience
As hinted before, this model does not attempt to solve the “hard problem” of consciousness —
nor does it require subjective feeling (qualia) as a criterion.
Consciousness, here, is not framed as something you have when experience feels vivid, but as
something that emerges structurally when a system filters change in defence of a projected self.
If subjective experience arises from that process, it is a by-product, not the threshold. The
model focuses not on what it feels like to be a system, but what types of systems can even lead
to the phenomenon of ‘consciousness’
Given the above, our proposal (while not mathematically formal), becomes testable at the very
least. This is because our theory is not dependent on internal experience to be valid, it only
needs observable structure.
It provides a behavioral scaffold for evaluating consciousness without appeal to mystery. We do
not look at what the system says it is doing or thinking, but how much it seems to care when
confronted with change. While not becoming fully falsifiable, our model moves us closer to
testability by asking “how many changes does a lifeform track, and for how long?” vs “what
does it feel or experience”
2. The origins of consciousness: an increased ability
to ‘care’
In this section, we further investigate the claim that the phenomenon perceived as
consciousness can be viewed as an increase in care towards the ‘self’. And we go further by
specifying two axes of ‘care’ that are primary responsible for many observations we associate
with ‘consciousness’:
- Caring about more types of change
- Caring about longer time periods of change
Consciousness, in this framework, is nether binary, nor is it sudden. It is best thought of
spectrum that increases as a system’s ability for self-care expands (along the two axes
mentioned above). These expansions of care are not metaphorical, rather they are structural
upgrades that come about through evolution11
. But before we get into the mechanism, let’s flesh
out what we mean by greater expansion along the two dimensions we have identified.
Expansion 1: Caring about more types of change
Even simple lifeforms contain a dizzying amount of diversity – from the ecological niche they
occupy to the strategies they deploy in keeping the ‘inside’ preserved.
But there is one thing common to all forms of life that are classified as simple – their boundarymechanism aka ‘self-identity’ is primarily defined by direct, measurable components:
membrane integrity, ionic gradients, DNA stability. It’s a lot of physical forces and chemistry,
and little else. So, it is not surprising, that the type of ‘harmful change’ they filter against is also
either physical or chemical in nature. The sheer physicality of their self-identity means that the
type of ‘consciousness’ they possess is primarily concerned with bodily integrity. For example, a
bacterium protects its genetic material and metabolic machinery using a semi-permeable cell
wall that blocks toxins and manages osmotic balance. Even unicellular algae or protozoa,
though more complex, rely on organelles that respond only to physical signals - nutrient
availability, chemical gradients, light exposure, or surface contact.
But over time, some life-forms emerge for whom self-identity is more than physical
components. While it is impossible to ask a toad or a dog whether it considers itself more than
just a physical body, we’re can make educated guesses. Based on the basic logic of care, a way
to understand a life-form’s sense of ‘inside worth protecting’ is to look at the types of change it
is protecting itself against. As we move away from simple lifeforms, we notice that lifeforms
begin to develop sensitivity to some non-physical, non-chemical disruptions as well. These may
include (but are not limited to):
- Social-Relational threats: Ones that harm the continuity of relationships, roles or
status within groups. E.g. a chimpanzee who shows withdrawal and stress when its
position in the group is threatened, as if defending a symbolic place in an abstract
structure.
- Moral-Value threats: Ones that go against deeply held beliefs. E.g. a person making a
costly decision to maintain internal alignment with a value system, even at the expense
of physical well-being.
- Symbolic-Identity threats: Ones that harm abstract models and narratives associated
with the self. E.g. humans willing to kill and die for a nation-state border, i.e., an
imaginary thing that is nonetheless felt as extremely real.
The changes above often do not pose immediate harm to the body, in the same way that a large
falling rock or a poisonous puddle would. But they do threaten the continuity of the ‘inside
world’ and are taken as seriously.
11 We cover the mechanisms of expansion, along with important nuances, in greater detail in the next
section.
DIGRESSION: On the circularity of identity formation and change-detection
Our discussion on LifeOS imperatives implied that the self-identity aka ‘inside world’ is created
first, and only after that does change filtering happen. But that explanation was presented to
ease up the cognitive load on an unsuspecting reader.
In reality, we suspect12 that the two steps have a chicken-and-egg sort of dynamic. That is to say
that it is possible that our explanation of a pre-existing structure that then defends against
change is too linear. The system could in fact emerge as it is defended. In this set-up the
system doesn’t filter because the self is already defined. The self becomes defined because
certain threats trigger recursive protection logic.
At first glance, the idea of a “self” that emerges by defending itself might sound suspiciously
circular. But this kind of loop is defensible and observationally grounded across multiple
domains:
- In infant psychology, identity coalesces not from abstract introspection, but from
repeated interactions with care-givers
- In narrative psychology, personal identity is built by selecting which memories feel worth
reinforcing. This isn’t because the self is fully known, but rather because certain
disruptions are unconsciously flagged as threatening to coherence.
- In biology, even simple organisms exhibit boundary-enforcing behavior long before any
complex identity map emerges. They resist what breaks internal order, and that
recursive resistance becomes their operational identity.
Our point (made by others in different contexts, perhaps most famously by Douglas Hofstadter)
is that recursive loops are not a logical flaw — they may be the birthing mechanism of
complexity itself. The self doesn’t have to be known in advance, it emerges from the act of
defending what parts of a shouldn’t be lost. This same logic applies to institutions, machines,
and even social roles: coherence begins to harden precisely at the moment a system resists
disruption in a patterned, recursive way.
NOTE: Having said that, no one fully understands (yet) how the transition occurs from purely
physical boundaries preserving internal structure (like long-chained molecules) to living
boundaries capable of recursive self-maintenance and filtration occurs. To the author, this
represents a critical area for future knowledge generation. Some promising hypotheses have
been explored in origin-of-life research, including work on autopoiesis (Maturana & Varela),
protocell models, and prebiotic structural coupling, but the leap from chemistry to recursive
care remains an open frontier.
Expansion 2: Caring about longer time periods of change
There is actually another common theme that runs through all simple lifeforms: they behave
reactively and can only seem to ‘care’ about the present moment. But as sophistication helps
lifeforms filter for more types of change, it also helps them filter changes for longer.
12 This is currently an open and fascinating area of research.
This makes intuitive sense too because many of the new changes that lifeforms filter against
(relational, symbolic, moral etc.) have longer cause-and-effect cycles than physical threats,
which tend to be overwhelmingly immediate. But the net impact is that complex lifeforms will
often begin to act on behalf of projected self-identity.
Interestingly enough, such lifeforms may even make sacrifices in the present to protect a
projected (not-yet-real) version of their inner-world. If this is hard to imagine, think about a
time you decided not to lie, even if lying would have been the easy option and even when there
was no chance of getting caught. It is likely that you did so to maintain your own narrative
identity of an honest human over time. No worm will avoid cheating to protect its narrative selfidentity.
And lest the reader think such future projection only happens with more abstract change (i.e.,
narrative cohesion), that is decidedly false. Complex lifeforms can simulate physical harm over
longer time periods as well, and take action accordingly. This is the reason so many people give
up on their vices, be it alcohol or meat or smoking, they often realize that their behaviours are
leading to the destruction of a projected self-identity.
Our overall point here is that the ability to simulate harm and adjust the ‘now’ to protect against
simulated future harm later is a turning point. Like increasing the types of change we care
about, sophistication pushes systems to symbolically persistent in the future — and its change
filtration mechanisms adjusts accordingly.
DIGRESSION: On the perception of ‘time’ by a system
Some system theorists might wonder: doesn’t this model treat time as linear? Don’t we know
that systems often experience time nonlinearly, often compressing, freezing, dissociating, or
looping?
The answer is that our model concerns itself with time-as-continuity-risk, not time as felt
duration. We care about the ability to simulate a future self that must be preserved, even if time
is experienced in fragments.
---
Let’s take a step back and clearly state our take-away: one of the most damaging habits in
discussions of consciousness is the belief that it either exists or it doesn’t - that it is binary.
This belief comes in many forms: either a system is “aware,” or it isn’t; either it has qualia, or it
doesn’t; either it passes a test, or it fails.
But this binary framing is a distraction. It forces us to treat consciousness like a light switch,
when in fact, it behaves more like a dimmer knob tied to structural care. Furthermore, when we
turn the dimmer knob to the right, we are essentially increasing (a) the types of threatening
change and (b) the duration of threatening change that is perceived. As simple as that.
Let’s see what happens when we apply this theory to the real world. We consider five different
entities: a bacterium, a worm, a dog, a chimpanzee, and a human. Most people would likely
agree that ‘consciousness’ would emerge as we move to the right. A few of us would even agree
that ‘consciousness’ increases as well move to the right. But in a world of qualia alone, such
intuitions are difficult to justify. After all, no one can say with certainty that a dog’s experience of
gnawing on a bone is materially inferior to you eating your mother’s best cooked dish.
But if we move to the layer of ‘structural care’, and especially the expansion of said care along
the two axes, the landscape becomes clearer.
Entity Change-types Time-duration Gut-feel on
consciousness
Bacterium Physical/Chemical Millisecondsseconds
None to low
Worm Physical/Chemical Seconds to minutes Slightly conscious
Dog Physical/Chemical +
Relational/Social
Hours-weeks (can
anticipate &
remember)
Moderately
conscious
Chimpanzee Physical/Chemical +
Relational/Social +
Moral/Value
Weeks-years (often
show long-term
recall)
Highly conscious
Human Physical/Chemical +
Relational/Social +
Moral/Value +
Symbolic/Abstract
Years – decades Fully Conscious
This table isn’t trying to measure how something feels on the inside. It’s a way of mapping how
different systems exhibit care for their self-identity. In that sense, it may be more useful than
traditional tests, because it tells us how many parts of the self are being defended, and how far
into the future that defence seems to extend.
- A bacterium filters for basic survival: pressure, toxins, acidity. It stays alive, but it doesn’t
seem to imagine anything.
- A worm avoids touch, pain, and certain patterns in space. It reacts, but it doesn’t appear
to carry a sense of self forward.
- A dog can remember emotional harm. It avoids people who’ve hurt it. It responds to
trust, hierarchy, and loss, filtering not just for safety, but for social continuity.
- A chimpanzee tracks status, reputation, fairness. It holds grudges. It comforts the
grieving. It filters for social identity and begins to model symbolic threats.
- A human can change their behaviour to protect a future version of themselves, even if
that version doesn’t yet exist. Sometimes, they sacrifice comfort now to stay loyal to a
story they believe they are becoming.
The table above may seem like an ode to human exceptionalism at first-glace, but that’s not
exactly accurate. If, like we propose, our intuitions on consciousness are built atop the
expansion of care, it is only natural to ask: are humans at the highest limits of
consciousness? Our framework suggests not.
When viewed from the angle of care, humans are simply one point on a X-Y gradient
corresponding to the care expansion. Sure, we’re a species with rich symbolic infrastructure, a
long time horizon, and a wide range of filtered disruption types, but that doesn’t guarantee
superiority. Even though we’re so much better at self-care than a chimpanzee or a worm, we’re
not perfect. We often sacrifice long-term coherence for short-term stimulation- for example,
think about the fact that very few bacterial strains would consume toxic substances for shortterm intoxication13
. We regularly ignore large-scale physical threats, like ecological collapse,
because they feel too abstract or temporally distant. We optimize for signals that are
immediate, social, or emotionally resonant, even when they contradict long-range continuity.
We are very conscious, but we are often impulsive, symbolically incoherent and still presentbiased.
Therefore, it’s not absurd to imagine another system that can do a better job of self-care. A
system, be it biological, artificial or hybrid, that cares about even more types of change and for
longer periods of time. An example of such care might extend to concern about long-term
ecological health of this planet, something that appears to be beyond practical human abilities.
The point is that our place as the ‘most conscious’ intelligence under this model isn’t
guaranteed.
DIGRESSION: On positive connotations associated with ‘high consciousness’
It is important, at least to the author, to surface an oft-unquestioned assumption masquerading
as self-evident truth: that ‘higher’ states of consciousness are always preferable. Just like
ranking of the five entities, many people might find themselves agreeing with this assumption
but are unable to say why.
Is it the belief that greater consciousness will somehow lead to better outcomes for everyone?
Or is it something simpler - that the subjective feelings of control and agency that expanded
change filtration brings will always feel great to the individual lifeform.
We suspect it’s the latter, and humans themselves are a great case-study in proving our point.
Many would admit that humans have a high degree of consciousness. And while most of us
wouldn’t dream of giving that up (why would anyone give up agency, reflection, and long-loop
awareness); our report card at the species level is…well, it isn’t great. We’re doing
catastrophically when it comes to changes that have exceptionally long cause-effect cycles.
For example, on the ability to extend care to the broader ecosystems we depend on, our
species is failing miserably. Now compare that to bacteria which scores very low on any
plausible scale of consciousness. And yet, collectively, bacteria have maintained Earth’s
habitability for billions of years without coming anywhere close to a scenario of selfperpetuated extinction. Humans, on the other hand, seem to be adding more and more
candidates that could lead to our own extinction (nuclear weapons, climate change, bioengineered weapons etc.).
Bacteria as a species haven't triggered mass extinction events. They haven't overheated the
biosphere. Mostly because they cannot do these things without an expanded sense of care to
begin with, which brings us to an uncomfortable conclusion: that higher levels of
consciousness may feel good to the agent — but can pose catastrophic risk to the system.
Let’s take a specific example and really crystallize this point. Take the example of Pol Pot. Once
the idea of a “purified agrarian future” became entangled with his symbolic self-model, he
13 It is very likely they do not have the biological hardware to even experience ‘intoxication’.
filtered everything through it - modernity, dissent, even education, each an existential threat to
the future he imagined. And so, he acted in long-loop defence of that narrative: rebooting
society to “Year Zero.” Erasing the present to protect the purity of a projected structure. We can
guess that to Pol Pot at least, it must have felt like a grand culmination of his life’s mission. He
probably even had a great time setting up the Khmer Rouge. However, it doesn’t take a genius to
agree that this probably wasn’t great for the ~2 million people who died in the process. Or even
for Cambodia - after all, losing 25% of its population in under 4 years has never been good for
the competitiveness of a nation-state.
So, we should be careful what we glorify. Greater consciousness often feels good to the agent;
but feelings can be deceptive. Not all awareness leads to wisdom; not all self-preservation
leads to survival.
3. The mechanism behind it all
We hope that by this point, the reader finds some merit in the argument proposed. The next
obvious question becomes – how does all of this expansion of ‘care’ actually happen?
We do not propose any novel mechanism for it – we do not need to. Everything we claim
emerges naturally through known theories of evolutionary biology. The core link between
evolution and our discussion is the insight that the kinds of systems (read: species) that survive
are those that become better at recognizing which disruptions actually matter, and more skilled
at protecting their internal structure from collapse.
Evolution is the main engine behind this process of becoming better at recognition, and more
skilled at protection. Most biologists would agree that adaptive fitness and change are
intricately linked – the species that often survive are ones for whom the internal change (i.e.,
mutation) is perfectly suited for external change (i.e., changes in the environmental). Our logic
simply reframes this dynamic; ‘care’ is not a co-incidence, it has to emerge if evolutionary
theory is to be believed.
Similarly, expanded care must also emerge as evolution forces experimentation that often leads
to more sophistication in change-detection and ‘inside world’ creation. We do have some prime
candidates that appear to tie physiological organs to this conceptual ‘expanded care’. Let’s
cover two important organs that illustrate how expanded care emerges directly from
sophisticated evolutionary physiology:
The Prefrontal Cortex: Delaying Action to Protect Self-Structure
We hear a lot about the prefrontal cortex (PFC), most commonly about how it allows humans to
do abstract thinking. What we often don’t realize is that many of the abstract thoughts that arise
in the PFC become a core part of our ‘inside world’ – things like religion, codes of conduct,
social norms etc. It is not a stretch to claim that our ability to perceive increasing types of
abstract change (moral, relational etc.) is tied to organs that allow for creation and manipulation
of abstract things.
Amazingly, that’s not it. The PFC is involved in both the expansions of care, and as part of its
complementary role, it balances the tension between what we want now vs what we want in
the future. In the language of care, the PFC is what allows long-loop harm vs short-term benefit
to play out. It may occasionally (or even often) get things wrong, but the point is that these longloop vs short-terms trade-offs are at least taking place – the same can’t be said for insects.
The introduction of the PFC represents a significant structural upgrade in what LifeOS can do.
Instead of acting on raw stimulus, the LifeOS begins filtering for long-loop harm across many
different change types, this includes very abstract things such as status, integrity, relational
trust, or narrative self-consistency. In this framing, the PFC plays a central role in protecting
self-identity across time. The PFC we’ve referred to is, of course, human specific, but humans
aren’t the only species with structures that perform similar functions. Chimpanzees14
,
elephants15, crows and ravens16 are some creatures that also seem to have structures that
behave similarly.
Default Mode Network: The Engine of Simulated Identity
For our next organ, let’s focus on the enigmatic Default Mode Network. Here’s something
strange about it, under MRI scanners the DMN lights up not during tasks, but in the spaces
between them: when a person is idle, daydreaming, or reflecting. The time you spend daydreaming may seem like a total waste, but it is an important element in the expansion of care. It
is precisely in such moments that a lifeform simulates itself. It replays past actions, imagines
future consequences, rehearses conversations, and evaluates moral alignment. This matters
especially in the expansion of care across time. A system’s ability to respond to harmful change
can be severely improved if it is allowed to simulate. The DMN allows the self-identity to be
rendered in pixel-perfect clarity: not just the body, but the role, context and imagined future too!
In LifeOS framing, the DMN doesn’t create consciousness – no one organ alone does, but it
enables the recursive logic that consciousness demands. Without it, a system might still feel
pain, or make decisions, but it likely wouldn’t simulate the consequences of its own collapse —
and it wouldn’t care about symbolic erosion before it happens. The DMN, then, is not a bonus
feature, rather it is a structural necessity for high-fidelity, projected self-preservation.
Again, while our discussion has been quite specific to humans; other highly conscious animals
such as dogs17, rats18 and chimpanzees19 too have brain structures that perform similar roles,
although perhaps not to the same extent as a human DMN.
----
We have covered two of the more well-known organs that deal with change filtration and selfprojection in humans, but there are many more specialized organs that contribute to this
expanded care. Instead of writing a biological treatise on the topic, it is more interesting to think
through some implications that must emerge from the mechanisms we’ve hinted at. We will
focus on one such implication for the rest of this section.
14 Beran et al., 2004, “Chimpanzees (Pan troglodytes) can wait for delayed rewards.”
15 Hart et al., 2008, “Cognitive behaviour in Asian elephants: use and modification of branches for fly
switching.”
16 Kabadayi & Osvath, 2017, “Ravens parallel great apes in flexible planning for tool-use and bartering.”
17 Berns et al., PeerJ, 2015 — “Functional MRI in awake unrestrained dogs suggests a default-mode-like
network.”
18 Lu et al., NeuroImage, 2012 —“Rat brains also have a default mode network.”
19 Rilling et al., PNAS, 2007 — “A default mode of brain function in monkeys and its relation to selfreflective cognition.”
But first, we must go back to how life-forms define their ‘internal worlds’ and then expand care
along two axes. Our discussion will be relevant to both individual (a baby transitioning into an
adult), and the species (a primitive organism evolving into a more sophisticated one). Our first
point is concerned with identity formation (at both individual and species level): the self is
defined in response to external change – and external change often has a random,
unpredictable quality to it. Our second point is concerned with expansion of care, and the fact
that it is heavily dependent on evolution. Guess what, there is a random, co-incidental quality to
evolution as well.
The overarching point is that there is a certain level of randomness with both the creation of self,
and the expansion of care. This shouldn’t be too hard to swallow considering we live in a pretty
random world. But it has an interesting implication on self-identity– especially for ‘highly
conscious creatures tracking lots of different changes for long time periods.
The net effect of all this randomness is that ‘highly conscious’ organisms define their selfidentity or ‘inner world’ in an almost haphazard manner. There is certainly some overall sense of
coherence, but the ‘inner world’ is more akin to a mosaic of different images rather than one
single drawing. In other words, the self-identity of highly conscious beings inevitably becomes a
sort of emergent phenomenon of many different mini- identities interacting with each other.
Here’s why it matters: it is often the case that these mini-identities may not align with each
other on the type of change to be avoided (or the duration for care to be deployed). Instead,
they are shaped by survival needs in specific environments. Since these mini-identities arise in
specific contexts and are triggered in those contexts, thankfully there isn’t too much confusion
most of the time. But in certain situations, two or more mini- identities might get triggered
simultaneously. And in a smaller sub-set of cases, they might have very different definitions of
what type of care to enact. And when they do, the system faces a problem: it can’t just average
different definitions of care: it has to choose one over the other.
We propose that this conflict is managed by a process that, for lack of a better word, can be
called Identity Arbitration. Most highly conscious beings would have something like it - a
structural mechanism that helps a system rank and prioritize when conflicts between ‘inner
worlds’ arise. This too isn’t some magical phenomenon, but deeply rooted in evolutionary
physiology. In humans, this arbitration can be said to take place across several interlocking
brain systems. For example, the Anterior Cingulate Cortex (ACC) — detects conflicts between
active goals and flags need for behavioral change, in simpler language it’s the part that says
“something’s not right” when your actions don’t match your long-term goals. The Dorsolateral
Prefrontal Cortex (DLPFC) inhibits reactive behaviors to preserve longer-term identity values, or
it lets you stop yourself from saying something hurtful, because it’s not who you want to be.
Ventral Hippocampus (vHPC) tags memories with emotional relevance, shaping approachavoidance tradeoffs, or simply put it helps you remember how certain actions made you feel, so
you don’t repeat painful patterns.
These brain-systems don’t erase contradictions, rather they let the lifeform survive it. They allow
a unified self to persist, even when facing potential rebellion. Because of the inherently random
nature of change (both in mutation and in the environment), lifeforms that extend care will face
rebellion from time to time. This is likely true of any sophisticated organism where mosaiced
‘parts’ are forcefully assembled in a trial-and-error bottom-up manner, without any grand
architectural top-down design20. In fact, while we’ve only covered humans in this discussion,
the identity arbitration is observable in other species as well. Take the case of chimpanzees who
have been observed suppressing aggression toward group members in favor of long-term social
cohesion, even when revenge is immediately available21
.
We end this section with the tacit acknowledgement that to be highly conscious, then, is not
merely to defend complex identity across time, but also to resolve contradiction in favor of
continuity. It is tied to the ability to filter between possible selves — and to select the one
that best preserves symbolic structure over time.
4. What’s missing in the current discourse
We recognize that this proposal might appear strange in the context of current mainstream
discourse. However, it is important to remember that what we have discussed doesn’t
contradict any theory like IIT, GWT or FEP. Instead, it makes each one sharper, and adds missing
context. The reason that it is able to do that for each theory is because all of them suffer from
the same flaw: a lack of exploration for why things appear the way they do.
Each theory deals with what’s happening inside a system when it’s awake, aware, or behaving
like a mind. These models are often elegant, mathematically rich, and scientifically valuable –
their usefulness evidenced by the face that they’re already used in practical applications, from
anesthesiology to AI research. But for all their complexity and elegance, they fail to answer a
simpler question: what structural purpose does consciousness serve?
And without attempting to understand the structural reasons, they will always be unable to
answer certain questions. Making these theories even more elegant and mathematically
nuanced will not be enough to over overcome this problem. If you’re travelling without a
compass, making your ship go even faster doesn’t guarantee earlier arrival.
Integrated Information Theory (IIT)
Developed by Giulio Tononi and colleagues, IIT proposes that consciousness arises from
integrated information, specifically the degree to which a system’s internal states form
irreducible cause-effect structures. This yields a metric, Φ (phi), that claims to quantify “how
much” consciousness a system contains.
On paper, this is conceptually elegant. It recognizes that fragmentation destroys continuity, a
point many overlook. But here’s where it drops the structural purpose thread – the
continuity of what exactly? The framework makes limited, if any, attempts to locate a
preservation logic inside the system - there are no parallel concepts of care within the theory, no
organizing boundary-mechanisms for self-identity, no stake in self-continuity. This leads us to
seemingly strange places, a refrigerator full of self-checking circuits might have high Φ. Most
sane people (including even panpsychists) would not classify such a refrigerator as highly
conscious. This blind spot makes it difficult to use IIT to distinguish true self-modeling systems
from complex, reactive ones. IIT is useful in telling us which systems are ‘on’, but not if they’re
fighting to stay that way.
20 Which is exactly how evolution behaves
21 Conflict monitoring and cognitive control by Botvinick et al. (2001)
And crucially, it is this ‘fight to preserve’instinct that is at the heart of most moral and
ethical discussions around consciousness; the ability to ‘act like a tightly integrated system’ is
not ethically important for most of us.
Global Workspace Theory (GWT)
Pioneered by Bernard Baars and Stanislas Dehaene, GWT is a reasonable competitor to IIT.
Instead of cause-and-effect structures, GWT looks for information that is globally accessible
across subsystems. This abstract area where globally accessible information resides is called
the “workspace.” Similar to IIT, GWT has been useful in helping us understand specific
instances of ‘conscious behaviour’ such as when systems pay selective attention to information
around them. Under GWT this arises from a competition between internal signals, with the
winner gaining access to the ‘workspace’. Or take the case of executive control which suggests
that something enters the workspace, it can influence different sub-systems such as planning,
and memory, and motor systems. For instance, if a threat enters the workspace (“that stove is
hot”), it can override many ongoing processes and force systems to collectively respond to this
threat.
This too is an interesting mechanism, but it fails to explore why certain data enters the
workspace to begin with? It treats global availability as equivalent to salience, without
questioning who, or what, or why that information is being elevated to such importance.
Perhaps an even bigger critique of GWT is that doesn’t handle long-loop logic adequately,
often treating all conscious systems as reacting to the workspace of the ‘now’. So, we get a
system that broadcasts, but not one that prioritizes symbolic self-preservation across time. This
leaves us especially vulnerable to performance traps in understanding systems: mistaking
eloquence for identity, and imitation for introspection.
Free Energy Principle (FEP)
We arrive at our third contender, Free Energy Principle advanced by Karl Friston. Of the three
mainstream models, this one comes closest in spirit to the theory proposed in this essay. It says
that conscious systems aim to minimize prediction error (aka ‘free energy’) by aligning internal
models with sensory inputs. It has been tremendously useful in modelling how systems selfcorrect, update, and stabilize against chaos; helping us make significant advances in biology,
neuroscience and machine learning.
As mentioned, it is close to our own theory of care, and yet it misses something important: it
equates all surprise reduction with care. In essence, it’s the difference between suggesting
that lifeforms filter for harmful change (our theory) versus the much stronger assertion of FEP
that lifeforms regard all change as harmful - all change is a surprise after all. Furthermore, even
in the case of treating all change as harmful, it doesn’t offer mechanisms for cases where a
systems is forced to choose between a lesser of two evils (read: it can only minimize one of two
surprises). In effect, FEP explains how systems adapt and update, but not what they protect
and filter. As such it fails in situations where surprise minimization is not important – things like
novelty-seeking behaviour or even signing up for psychotherapy are aspects of long-term care
even if they involve generating surprising information.
DIGRESSION: A note on structurally adjacent theories
As mentioned in the first section, there are a smaller set of theories like Autopoiesis (Maturana
& Varela)22, Enactivism (Varela, Thompson, Rosch)23 and Viable System Theory (Stafford Beer,
Danko Nikolic)24 that are stronger matches to our theory of care, especially when compared to
the mainstream theories discussed above.
However, each of them suffers from the same type of flaw – each one treats structural
continuity as a metabolic or control characteristic. This design choice forces them to only
engage with physical/chemical change rather than any other change-type rooted in
relationships or symbolism.
Furthermore, just like FEP, they do not seem to acknowledge the reality of a system being
subjected to conflicting instances of change, and the consequent ‘choice’ a system must make.
Just like FEP, they are amazing at defining adaptation, but not care.
----
To be very clear: the critique here is not that these models are “wrong.” They are simply
orthogonal to the most important question: why does the system behave in the manner that it
does? That’s the question this model focusses on, not by postulating new math, but by
reframing consciousness as the protection of a projected identity structure across change-type
and time.
This small reframe, has large consequences for further investigating consciousness.
Specifically, it gives us a solid foundation to explore non-traditional intelligence that
confuses or confounds existing theories.
5. Putting the Model to Work: Non-Traditional Intelligence and
the Architecture of Care
As mentioned, one of the biggest advantages in the language of care is that it allows us to
investigate edge-cases atop a more stable foundation. Compared to the narrative theories that
focus on qualia or body cognition, we are able to make testable claim that are supported by
first-principles logic of care and evolution. On the other hand, when compared to more formal
theories such as IIT, GWT or FEP, we are able to exactly pinpoint why the theories’ prediction for
edge-cases do not match most people’s common-sense intuition.
22 Which, like our theory, proposes that living systems are defined not by what they are made of, but by
what they do. Under this framework, action is defined as their ability to continuously regenerate and
maintain their own boundaries. A system is alive if it produces the components that keep it distinct from
the world around it.
23 Which holds that perception and cognition don’t happen inside a brain in isolation, they emerge from a
system’s active, embodied engagement with its environment. The world isn’t just perceived; it’s enacted
through sensorimotor feedback loops.
24 Which models how a system maintains its integrity by recursively adapting to external change without
collapsing its internal structure. These adaptations are not static rules but dynamic, layered responses
aimed at preserving coherence under pressure.
To see this in action, this section will investigate three edge cases that are considered
‘inconvenient’ for the other theories we’ve discussed. Using the theory of ‘care’ as described,
we will investigate whether these cases are ‘conscious’ by asking:
1. Do they filter disruption in service of an ‘internal world’ aka self-identity?
2. How many different types of change do they filter?
3. Do they protect that structure recursively across time?
A. Machines – Recursion without identity
Artificial Intelligence has always been a confusing edge-case, but after LLMs burst on the scene,
the confusion has exploded. LLMs have shown us that they can process language, manipulate
complex symbolism, simulate empathy and even adjust their behaviour across session. These
are activities that only lifeforms with expanded care have traditionally been able to achieve. To
many, therefore, this represents the fact that these LLM models have achieved consciousness.
But to the author, this view is reminiscent of putting the cart before the horse. And the core
mistake is centred around mistaking intelligence for care. We define ‘intelligence’ to mean
improved pattern recognition with a view to improve outcomes25
. Under this definition, all care
will have elements of intelligence, but not all intelligence will have elements of care. In
fact, there’s a very strong condition for when we are allowed to equate intelligence with care -
this can only happen when the outcome being optimized is the protection of ‘internal world’. If
the outcome is primarily concerned with optimizing anything else, say greater user engagement
or NPS scores, it cannot be counted as care.
Under our theory then, the more important ethical question for machines isn’t whether they can
act like living creatures; it’s whether they seem to protect a coherent internal world and resist
symbolic collapse. Most machines that are commonly deployed fail to meet that criteria. They
certainly adjust based on feedback, but they do so without internal self-modeling. They
preserve coherence for user engagement, but do not care about their own symbolic selfidentity26
. Have you ever wanted to pull out your hair because of the ‘comfort’ChatGPT displays
in being totally inconsistent with its answers, even over a 5 minute period? Perhaps this
behaviour is more comprehensible now - the feeling of deep discomfort with inconsistency over
short periods of time is rooted in care (and survival), not intelligence (outcome optimization)
alone. ChatGPT is surely intelligent, but it does not care.
However, this debate has probably not run its entire course. There is no reason to think that
future machines too will do the same. Instead, it is more likely that machines will eventually be
able to reorganize themselves to protect their own perceived and adjusting ‘internal world’
across time. At that point, artificial intelligence will start to become natural care. We don’t know
25 Broadly, one could say that a strategy that leads to better outcomes is one that is either (a) wrong by
lesser amounts compared to alternatives, (b) wrong less often compared to alternatives, or (c) takes
lesser effort to get to the same output compared to alternatives. LLMs appear to kill it (from a user’s
perspective) on the third aspect mentioned.
26 Anyone who has pulled out out their hair because of how comfortable LLMs ‘feel’ when being totally
inconsistent in their answers can now understand why this happens. The feeling of deep discomfort with
inconsistency over short periods of time is rooted in care, not intelligence.
if and when that could happen, but at least we know what to look for before re-opening this
endless debate.
B. Sociopathy – Consciousness without compassion
We covered the absurdity of Pol Pot and how highly conscious agents can be catastrophic for
the wider group (e.g., nation or species). While dictators are an extreme example, the point was
more broadly applicable to things we usually refer to as sociopathy or lack of empathy.
Under the definition of ‘care’, kindness is not a pre-requisite for consciousness27
. And
sociopaths are a case in point. Much like Pol Pot, sociopaths28 often demonstrate strong
symbolic identity, a large set of tracked change-types and long-loop preservation. What’s
missing is not exactly ‘care’; in fact, the problem may be that there is too much self-care
involved.
In most humans, the ‘inner world’ has been shaped by evolution to allow for basic empathy that
promotes co-operation and survival of larger groups. So, over time the ‘inner world’ or ‘selfidentity’ has been adapted29 to include the well-being of associates as well – usually friends,
family or tribe-members. Unfortunately, the ‘inner world’ of sociopaths seems to be faulty in this
regard. While a sociopath may identify with certain narrative and work to preserve their own
symbolic ideas across time, they will consistently fail to include the people around them in their
circle of ‘care’. In fact, it is this specific behaviour that we call ‘sociopathic’. Paradoxically, it’s
not that sociopaths don’t care, it’s that they don’t care about others30
.
C. Collectives – Consciousness without qualia
Insect and ethology circles have always been strange to the author – there is a lot of debate on
whether collectives such as ant colonies are alive or not. With people like E.O. Wilson, Thomas
Seeley and Deborah Gordon saying they are; in opposition to people like Richard Dawkins, Peter
Godfrey-Smith who take the opposite position. That’s not strange in itself, what’s strange is that
this is where the debate stops – neither side will ever claim that these collectives are conscious.
Presumably, that’s too absurd for any serious thinker to consider. But why?
When looked at from the lens of care, a different picture emerges. Collectives, a broader term
that goes beyond insects to include human tribes and groups, can be said to display some signs
of care. This is not to say most collectives pass the mark – the opposite statement is more likely
to be correct. But there are certain collectives such as religions, cults, tribes that exhibit many
characteristics of care such as symbolic threat detection, identity based decision-making, and
recursive conflict resolution. As such, these collectives could represent some form of protoconsciousness.
Of course, if one insists on the presence of subject experience, these systems do not quality as
they are usually symbolic but not sentient themselves. While they may defend narrative
integrity, they do not feel violated. To the author, this too is a valid take; it just uses a different
threshold level to the one we are proposing. Even under the qualia-first consideration, our
model provides a floor – after all, qualia can only emerge once a structure for care has been
27 Even if the author wishes, like the reader, that it were
28 A group that regularly includes leaders of every field, from politics and business to art and culture
29 This is likely true of most species for whom co-operation is a strategy,
30 It is more accurate to say, ‘they care even less about others than most strangers – who can be counted
on to follow societal norms of sociability’
established. While the choice of threshold may be an individual preference, the implications of
such a preference are much wider.
We often treat collectives as legal agents, but not as moral ones; and this is primarily because
they don’t feel conscious to us. But in a reality where collectives have a much greater impact
than any single human, will adopting a care-first threshold be better suited to bring about a
sustainable future?
Closing Thoughts
And so, we arrive at the conclusion. The arguments presented have sought to convince the
reader that even though consciousness is not yet understood, it is also not some inscrutable
mystery it’s often made out to be. It can be easily understood as an emergent phenomenon
related to care: when a system begins to recognize an ‘inner world’and act to defend it
across different threats and time periods.
This is why “I care, therefore I am” is not simply a metaphor, rather it is the signature of
consciousness itself - not a light that magically turns on, but a spectrum under which ‘care’
becomes increasingly abstract and long-looped.
For better or worse. 